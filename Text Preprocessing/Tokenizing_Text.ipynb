{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3_nZRdzAYQr"
      },
      "source": [
        "#  Tokenizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agIwFjiDAYQt"
      },
      "source": [
        "Fundamental step in NLP involves converting our text into smaller units through a process known as tokenization. These smaller units are known as our tokens. Word tokenization is the most common form of tokenization, where individual words in the text becomes a token, but tokens can also be sentences, sub words or individual characters depending on your use case.\n",
        "\n",
        "The meaning of the overall text is better understood if we can analyse and understand the individual parts as well as the whole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y--0VlejAYQu",
        "outputId": "bf7a32bc-7282-4e79-e235-32bfcf35bd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ra-K88pAYQv"
      },
      "source": [
        "### Sentance tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJG7voKWAYQv",
        "outputId": "a6f802fd-63ee-4836-91dd-91c175ee1fba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Her cat's name is Luna.\", \"Her dog's name is max\"]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "sentences = \"Her cat's name is Luna. Her dog's name is max\"\n",
        "sent_tokenize(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG06PiYkAYQw"
      },
      "source": [
        "### Word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnNASmUoAYQw",
        "outputId": "4f3c84ed-50dd-459d-f1e7-2a31710fa81e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Her', 'cat', \"'s\", 'name', 'is', 'Luna']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "sentence = \"Her cat's name is Luna\"\n",
        "word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmWsi0irAYQx"
      },
      "source": [
        "Notice how \"cat's\" has been split into 2 tokens. This may be fine for your task but it is definitely something to keep in mind when you are preprocessing any text data - you might want to remove punctuation or replace contractions before tokenizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1DUgtlxAYQx",
        "outputId": "cfffeb46-3485-4963-c02e-dfb17f733fea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Her',\n",
              " 'cat',\n",
              " \"'s\",\n",
              " 'name',\n",
              " 'is',\n",
              " 'Luna',\n",
              " 'and',\n",
              " 'her',\n",
              " 'dog',\n",
              " \"'s\",\n",
              " 'name',\n",
              " 'is',\n",
              " 'max']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "sentence_2 = \"Her cat's name is Luna and her dog's name is max\"\n",
        "word_tokenize(sentence_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdaHweOGAYQy"
      },
      "source": [
        " We can see we have two instances of the word 'her' - one which is capitalised. The tokens then are different and will be treated as different in most analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCct6p-uAYQ0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
